#!/usr/bin/env python3
"""
æµ‹è¯•æœ¬åœ°æ¨¡å‹é…ç½®çš„ç¤ºä¾‹

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºå¦‚ä½•é…ç½®å’Œä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- Ollama
- LocalAI
- vLLM
- è‡ªå®šä¹‰ OpenAI å…¼å®¹ API
"""

import asyncio
import json
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.core.llm_adapters import OpenAICompatibleAdapter


async def test_ollama():
    """æµ‹è¯• Ollama æœ¬åœ°æ¨¡å‹"""
    print("ğŸ¦™ æµ‹è¯• Ollama æœ¬åœ°æ¨¡å‹...")
    
    adapter = OpenAICompatibleAdapter(
        model_name="llama2:7b",
        endpoint="http://localhost:11434/v1/chat/completions",
        provider="ollama"
    )
    
    try:
        response = await adapter.chat_completion(
            messages=[
                {"role": "user", "content": "Hello! Please respond briefly."}
            ],
            temperature=0.7,
            max_tokens=50
        )
        
        print("âœ… Ollama å“åº”æˆåŠŸ:")
        print(f"   Content: {response.get('content', 'No content')}")
        print(f"   Model: {response.get('model', 'Unknown')}")
        
    except Exception as e:
        print(f"âŒ Ollama è¿æ¥å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")


async def test_localai():
    """æµ‹è¯• LocalAI"""
    print("\nğŸ¤– æµ‹è¯• LocalAI...")
    
    adapter = OpenAICompatibleAdapter(
        model_name="gpt-3.5-turbo",
        endpoint="http://localhost:8080/v1/chat/completions",
        provider="localai"
    )
    
    try:
        response = await adapter.chat_completion(
            messages=[
                {"role": "user", "content": "Hello! Please respond briefly."}
            ],
            temperature=0.7,
            max_tokens=50
        )
        
        print("âœ… LocalAI å“åº”æˆåŠŸ:")
        print(f"   Content: {response.get('content', 'No content')}")
        print(f"   Model: {response.get('model', 'Unknown')}")
        
    except Exception as e:
        print(f"âŒ LocalAI è¿æ¥å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ LocalAI æœåŠ¡æ­£åœ¨è¿è¡Œ")


async def test_vllm():
    """æµ‹è¯• vLLM æ¨ç†æœåŠ¡å™¨"""
    print("\nâš¡ æµ‹è¯• vLLM...")
    
    adapter = OpenAICompatibleAdapter(
        model_name="microsoft/DialoGPT-medium",
        endpoint="http://localhost:8000/v1/chat/completions",
        provider="vllm"
    )
    
    try:
        response = await adapter.chat_completion(
            messages=[
                {"role": "user", "content": "Hello! Please respond briefly."}
            ],
            temperature=0.7,
            max_tokens=50
        )
        
        print("âœ… vLLM å“åº”æˆåŠŸ:")
        print(f"   Content: {response.get('content', 'No content')}")
        print(f"   Model: {response.get('model', 'Unknown')}")
        
    except Exception as e:
        print(f"âŒ vLLM è¿æ¥å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œ")


async def test_custom_api():
    """æµ‹è¯•è‡ªå®šä¹‰ APIï¼ˆå¸¦è‡ªå®šä¹‰ headersï¼‰"""
    print("\nğŸ”§ æµ‹è¯•è‡ªå®šä¹‰ API...")
    
    custom_headers = {
        "X-Custom-Auth": "my-secret-token",
        "User-Agent": "ModelVS3/1.0"
    }
    
    adapter = OpenAICompatibleAdapter(
        model_name="custom-model",
        endpoint="http://localhost:9000/v1/chat/completions",
        provider="custom",
        custom_headers=custom_headers
    )
    
    try:
        response = await adapter.chat_completion(
            messages=[
                {"role": "user", "content": "Hello! Please respond briefly."}
            ],
            temperature=0.7,
            max_tokens=50
        )
        
        print("âœ… è‡ªå®šä¹‰ API å“åº”æˆåŠŸ:")
        print(f"   Content: {response.get('content', 'No content')}")
        print(f"   Model: {response.get('model', 'Unknown')}")
        
    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰ API è¿æ¥å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿æ‚¨çš„è‡ªå®šä¹‰ API æœåŠ¡æ­£åœ¨è¿è¡Œ")


async def test_tool_calling():
    """æµ‹è¯•å·¥å…·è°ƒç”¨åŠŸèƒ½"""
    print("\nğŸ”¨ æµ‹è¯•å·¥å…·è°ƒç”¨...")
    
    # ä½¿ç”¨ Ollama æµ‹è¯•å·¥å…·è°ƒç”¨
    adapter = OpenAICompatibleAdapter(
        model_name="llama2:7b",
        endpoint="http://localhost:11434/v1/chat/completions",
        provider="ollama"
    )
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "calculator",
                "description": "æ‰§è¡Œæ•°å­¦è®¡ç®—",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "expression": {
                            "type": "string",
                            "description": "è¦è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼"
                        }
                    },
                    "required": ["expression"]
                }
            }
        }
    ]
    
    try:
        response = await adapter.chat_completion(
            messages=[
                {"role": "user", "content": "è¯·è®¡ç®— 15 + 27 = ?"}
            ],
            temperature=0.1,
            max_tokens=100,
            tools=tools
        )
        
        print("âœ… å·¥å…·è°ƒç”¨æµ‹è¯•:")
        if response.get("tool_calls"):
            print(f"   å·¥å…·è°ƒç”¨: {response['tool_calls']}")
        else:
            print(f"   æ™®é€šå“åº”: {response.get('content', 'No content')}")
        
    except Exception as e:
        print(f"âŒ å·¥å…·è°ƒç”¨æµ‹è¯•å¤±è´¥: {str(e)}")


def print_setup_instructions():
    """æ‰“å°è®¾ç½®è¯´æ˜"""
    print("ğŸ“‹ æœ¬åœ°æ¨¡å‹è®¾ç½®è¯´æ˜:")
    print()
    print("1. Ollama è®¾ç½®:")
    print("   - å®‰è£…: https://ollama.ai/")
    print("   - å¯åŠ¨: ollama serve")
    print("   - ä¸‹è½½æ¨¡å‹: ollama pull llama2:7b")
    print()
    print("2. LocalAI è®¾ç½®:")
    print("   - é¡¹ç›®: https://github.com/go-skynet/LocalAI")
    print("   - Docker: docker run -p 8080:8080 localai/localai")
    print()
    print("3. vLLM è®¾ç½®:")
    print("   - å®‰è£…: pip install vllm")
    print("   - å¯åŠ¨: python -m vllm.entrypoints.openai.api_server \\")
    print("            --model microsoft/DialoGPT-medium --port 8000")
    print()
    print("4. è‡ªå®šä¹‰ API:")
    print("   - ä»»ä½•å…¼å®¹ OpenAI Chat Completions API çš„æœåŠ¡")
    print("   - æ”¯æŒè‡ªå®šä¹‰è®¤è¯å’Œ headers")
    print()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ModelVS3 æœ¬åœ°æ¨¡å‹é…ç½®æµ‹è¯•")
    print("=" * 50)
    
    print_setup_instructions()
    
    print("å¼€å§‹æµ‹è¯•æœ¬åœ°æ¨¡å‹è¿æ¥...")
    print("=" * 50)
    
    # æŒ‰é¡ºåºæµ‹è¯•å„ç§æœ¬åœ°æ¨¡å‹é…ç½®
    await test_ollama()
    await test_localai() 
    await test_vllm()
    await test_custom_api()
    await test_tool_calling()
    
    print("\n" + "=" * 50)
    print("âœ¨ æµ‹è¯•å®Œæˆï¼")
    print()
    print("ğŸ’¡ æç¤º:")
    print("- å¦‚æœæŸä¸ªæœåŠ¡è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¯¥æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
    print("- æ‚¨å¯ä»¥åœ¨ ModelVS3 å‰ç«¯ç•Œé¢ä¸­æ·»åŠ è¿™äº›æœ¬åœ°æ¨¡å‹")
    print("- æœ¬åœ°æ¨¡å‹é€šå¸¸ä¸éœ€è¦ API å¯†é’¥")


if __name__ == "__main__":
    asyncio.run(main()) 