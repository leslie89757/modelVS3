"""LLM é€‚é…å™¨æ¨¡å—"""

import asyncio
import json
import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
import httpx

logger = logging.getLogger(__name__)


class BaseLLMAdapter(ABC):
    """LLM é€‚é…å™¨åŸºç±»"""
    
    @abstractmethod
    async def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        tools: Optional[List[Dict[str, Any]]] = None,
        top_p: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None
    ) -> Dict[str, Any]:
        """èŠå¤©å®Œæˆ"""
        pass


class OpenAICompatibleAdapter(BaseLLMAdapter):
    """OpenAI å…¼å®¹é€‚é…å™¨"""
    
    def __init__(
        self, 
        model_name: str,
        endpoint: str,
        api_key: Optional[str] = None,
        provider: str = "openai",
        custom_headers: Optional[Dict[str, str]] = None
    ):
        self.model_name = model_name
        self.endpoint = endpoint
        self.api_key = api_key
        self.provider = provider
        self.custom_headers = custom_headers or {}
    
    def _build_headers(self) -> Dict[str, str]:
        """æ„å»ºè¯·æ±‚å¤´"""
        headers = {
            "Content-Type": "application/json",
            **self.custom_headers
        }
        
        if self.api_key:
            if self.provider == "anthropic":
                headers["x-api-key"] = self.api_key
            elif self.provider == "google":
                headers["Authorization"] = f"Bearer {self.api_key}"
            else:
                headers["Authorization"] = f"Bearer {self.api_key}"
        
        return headers
    
    def _build_payload(
        self,
        messages: List[Dict[str, Any]],
        temperature: float,
        max_tokens: int,
        tools: Optional[List[Dict[str, Any]]] = None,
        top_p: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None
    ) -> Dict[str, Any]:
        """æ„å»ºè¯·æ±‚è½½è·"""
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if top_p is not None:
            payload["top_p"] = top_p
        if frequency_penalty is not None:
            payload["frequency_penalty"] = frequency_penalty
        if presence_penalty is not None:
            payload["presence_penalty"] = presence_penalty
        
        if tools:
            payload["tools"] = tools
            payload["tool_choice"] = "auto"
        
        return payload
    
    async def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        tools: Optional[List[Dict[str, Any]]] = None,
        top_p: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None
    ) -> Dict[str, Any]:
        """èŠå¤©å®Œæˆ"""
        try:
            logger.info(f"ğŸš€ LLM APIè°ƒç”¨å¼€å§‹")
            logger.info(f"   æ¨¡å‹: {self.model_name}")
            logger.info(f"   ç«¯ç‚¹: {self.endpoint}")
            logger.info(f"   æä¾›å•†: {self.provider}")
            logger.info(f"   å·¥å…·æ•°é‡: {len(tools) if tools else 0}")
            
            headers = self._build_headers()
            payload = self._build_payload(
                messages, temperature, max_tokens, tools,
                top_p, frequency_penalty, presence_penalty
            )
            
            logger.info(f"ğŸ“¨ è¯·æ±‚è½½è·: {payload}")
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    self.endpoint,
                    headers=headers,
                    json=payload
                )
                
                logger.info(f"ğŸ“¥ APIå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code != 200:
                    error_msg = f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    raise Exception(error_msg)
                
                result = response.json()
                logger.info(f"ğŸ“‹ åŸå§‹å“åº”: {result}")
                
                parsed_result = self._parse_response(result, bool(tools))
                logger.info(f"âœ… è§£æåå“åº”: {parsed_result}")
                
                return parsed_result
                
        except Exception as e:
            logger.error(f"âŒ LLM APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _parse_response(self, response: Dict[str, Any], has_tools: bool) -> Dict[str, Any]:
        """è§£æå“åº”"""
        content = None
        tool_calls = None
        
        if "choices" in response:
            choice = response["choices"][0]
            message = choice.get("message", {})
            
            # å§‹ç»ˆå°è¯•è·å–contentï¼Œæ— è®ºæ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            content = message.get("content")
            
            # å¦‚æœå¯ç”¨äº†å·¥å…·ä¸”æœ‰å·¥å…·è°ƒç”¨ï¼Œä¹Ÿè·å–å·¥å…·è°ƒç”¨
            if has_tools and "tool_calls" in message:
                tool_calls = message["tool_calls"]
        
        return {
            "content": content,
            "tool_calls": tool_calls,
            "usage": response.get("usage", {}),
            "model": response.get("model", self.model_name)
        }


class LLMAdapterFactory:
    """LLM é€‚é…å™¨å·¥å‚"""
    
    def __init__(self):
        self._adapters = {}
    
    async def get_adapter(
        self, 
        model_name: str,
        endpoint: str,
        provider: str = "openai",
        api_key: Optional[str] = None,
        custom_headers: Optional[Dict[str, str]] = None,
        use_mock: bool = False
    ) -> BaseLLMAdapter:
        """è·å– LLM é€‚é…å™¨"""
        
        adapter_key = f"{model_name}:{endpoint}:{provider}"
        
        if adapter_key in self._adapters:
            return self._adapters[adapter_key]
        
        # åˆ›å»ºé€‚é…å™¨
        if use_mock:
            raise ValueError("æ¨¡æ‹Ÿé€‚é…å™¨å·²ç¦ç”¨ï¼Œè¯·ä½¿ç”¨çœŸå®çš„LLMé€‚é…å™¨")
        else:
            adapter = OpenAICompatibleAdapter(
                model_name=model_name,
                endpoint=endpoint,
                api_key=api_key,
                provider=provider,
                custom_headers=custom_headers
            )
        
        self._adapters[adapter_key] = adapter
        return adapter
    
    def clear_cache(self):
        """æ¸…é™¤é€‚é…å™¨ç¼“å­˜"""
        self._adapters.clear()


# å…¨å±€é€‚é…å™¨å·¥å‚å®ä¾‹
llm_factory = LLMAdapterFactory() 